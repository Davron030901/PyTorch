# -*- coding: utf-8 -*-
"""19.PyTorch Lightening Tutorial - Batch and LR Selection, Tensorboards, Callbacks, mGPU, TPU and more.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ywz6lm7Mr92uhU3-S76vj0XMkj-x6WCg

# **PyTorch Lightning**
### **PyTorch Lightning is an open-source Python library that provides a high-level interface for PyTorch, a popular deep learning framework.**
---
https://pytorch-lightning.readthedocs.io/en/latest/
---

In this lesson, we learn to use the amazing library **PyTorch Lightning**. It's a great way to organize. your PyTorch code and get many great features and added benefits. We'll be doing the following in this guide:
1. Setup and Install Lightning
2. Organzing our code into the Lightning structure/design philosophy
3. Automatic Batch Selection
4. Automatic Learning Rate Selection
5. Training using Lightning
6. Tensorboard logs
7. Callbacks - Early Stopping,  Checkpoints and using Lightning Bolts Metrics
8. Saving and Loading Models from checkpoints
9. Saving as Torchscript for Production Deployment
10. Inferences
11. Multiple GPU Training
12. Training on TPUs
13. Profiler for finding Bottlenecks in Training
14. 16-Bit GPU Training

## **1. Setup and Install Lightning**
"""

!pip install pytorch-lightning torchmetrics

# First we install PyTorch Lightning and TorchMetrics
!pip install pytorch-lightning --quiet
!pip install torchmetrics

import os
import torch
import torchmetrics
import torch.nn.functional as F

from torch import nn
from torchvision import transforms
from torch.utils.data import DataLoader, random_split

import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

from PIL import Image

"""#### **Download our datasets**"""

!gdown --id 1Dvw0UpvItjig0JbnzbTgYKB-ibMrXdxk
!unzip -q dogs-vs-cats.zip
!unzip -q train.zip
!unzip -q test1.zip

"""## **Setup our Dataloaders**"""

class Dataset():
    def __init__(self, filelist, filepath, transform=None):
        self.filelist = filelist
        self.filepath = filepath
        self.transform = transform

    def __len__(self):
        return int(len(self.filelist))

    def __getitem__(self, index):
        imgpath = os.path.join(self.filepath, self.filelist[index])
        img = Image.open(imgpath)

        if "dog" in imgpath:
            label = 1
        else:
            label = 0

        if self.transform is not None:
            img = self.transform(img)

        return (img, label)

# Set directory paths for our files
train_dir = './train'
test_dir = './test1'

# Get files in our directories
train_files = os.listdir(train_dir)
test_files = os.listdir(test_dir)

# Create our transforms
transformations = transforms.Compose([
    transforms.Resize((60, 60)),
    transforms.ToTensor()
])

# Create our train and test dataset objects
full_dataset = Dataset(train_files, train_dir, transformations)
test_dataset = Dataset(test_files, test_dir, transformations)

# Split into our train and validation
train_size = 20000
val_size = 5000
train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

"""## **2. Organzing our code into the Lightning structure/design philosophy**"""

# LightningModule implementation
class LitModel(pl.LightningModule):
    def __init__(self, learning_rate=0.02, batch_size=32):
        super().__init__()
        self.save_hyperparameters()
        self.learning_rate = learning_rate
        self.batch_size = batch_size

        # Model architecture
        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3), nn.ReLU(), nn.MaxPool2d(2, 2))
        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 3), nn.ReLU(), nn.MaxPool2d(2, 2))
        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2, 2))
        self.fc1 = nn.Sequential(nn.Flatten(), nn.Linear(64*5*5, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU())
        self.fc2 = nn.Sequential(nn.Linear(128, 2))

        # Metrics
        self.train_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=2)
        self.val_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=2)

    def forward(self, x):
        # in lightning, forward defines the prediction/inference actions
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.fc1(x)
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def training_step(self, batch, batch_idx):
        data, label = batch
        output = self.forward(data)
        loss = nn.CrossEntropyLoss()(output, label)

        # Calculate and log accuracy
        preds = torch.argmax(output, dim=1)
        acc = self.train_accuracy(preds, label)

        # Log metrics
        self.log('train_loss', loss, prog_bar=True)
        self.log('train_acc', acc, prog_bar=True)

        return {'loss': loss}

    def validation_step(self, batch, batch_idx):
        val_data, val_label = batch
        val_output = self.forward(val_data)
        val_loss = nn.CrossEntropyLoss()(val_output, val_label)

        # Calculate and log accuracy
        val_preds = torch.argmax(val_output, dim=1)
        val_acc = self.val_accuracy(val_preds, val_label)

        # Log metrics
        self.log('val_loss', val_loss, prog_bar=True)
        self.log('val_acc', val_acc, prog_bar=True)

        return {'val_loss': val_loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)

    def train_dataloader(self):
        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)

    def val_dataloader(self):
        return DataLoader(val_dataset, batch_size=self.batch_size, num_workers=2)

"""## **3. Automatic Batch Selection**"""

# Set up callbacks
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    dirpath='./checkpoints',
    filename='dogs-cats-{epoch:02d}-{val_loss:.2f}',
    save_top_k=3,
    mode='min'
)

early_stopping_callback = EarlyStopping(
    monitor='val_loss',
    patience=5,
    mode='min'
)

# Set up logger
logger = TensorBoardLogger("tb_logs", name="dogs_vs_cats")

# Initialize model
model = LitModel(learning_rate=0.01, batch_size=32)

# Initialize trainer with proper parameters for current PyTorch Lightning version
trainer = pl.Trainer(
    max_epochs=10,
    logger=logger,
    callbacks=[checkpoint_callback, early_stopping_callback],
    accelerator='auto',  # Automatically use GPU if available
    devices='auto',      # Automatically determine number of available devices
)

"""## **4. Automatic Learning Rate Selection**

Edit the Lightning Module as shown below. Note we've added in new lines at Line 5 to 8.
"""

import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchmetrics
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.tuner.tuning import Tuner  # Import the Tuner explicitly

class LitModel(pl.LightningModule):
    def __init__(self, learning_rate, batch_size):
        super().__init__()
        self.save_hyperparameters()  # Save hyperparameters
        self.batch_size = batch_size
        self.learning_rate = learning_rate

        # Fixed accuracy metrics with required 'task' parameter
        self.accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=2)
        self.train_acc = torchmetrics.Accuracy(task="multiclass", num_classes=2)
        self.valid_acc = torchmetrics.Accuracy(task="multiclass", num_classes=2)

        # Model architecture
        self.conv1 = nn.Sequential(nn.Conv2d(3,16,3), nn.ReLU(), nn.MaxPool2d(2,2))
        self.conv2 = nn.Sequential(nn.Conv2d(16,32,3), nn.ReLU(), nn.MaxPool2d(2,2))
        self.conv3 = nn.Sequential(nn.Conv2d(32,64,3), nn.ReLU(), nn.MaxPool2d(2,2))
        self.fc1 = nn.Sequential(nn.Flatten(), nn.Linear(64*5*5,256), nn.ReLU(), nn.Linear(256,128), nn.ReLU())
        self.fc2 = nn.Sequential(nn.Linear(128,2))

    def train_dataloader(self):
        return torch.utils.data.DataLoader(dataset=train, batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return torch.utils.data.DataLoader(dataset=val, batch_size=self.batch_size, shuffle=False)

    def cross_entropy_loss(self, logits, labels):
        return F.nll_loss(logits, labels)

    def training_step(self, batch, batch_idx):
        data, label = batch
        output = self.forward(data)
        loss = nn.CrossEntropyLoss()(output, label)

        # Log metrics
        self.log('train_loss', loss, prog_bar=True)
        self.log('train_acc_step', self.accuracy(output, label), prog_bar=True)

        return {'loss': loss}

    def on_train_epoch_end(self):
        # Modern replacement for training_epoch_end
        self.log('train_acc_epoch', self.accuracy.compute(), prog_bar=True)

    def validation_step(self, batch, batch_idx):
        val_data, val_label = batch
        val_output = self.forward(val_data)
        val_loss = nn.CrossEntropyLoss()(val_output, val_label)

        # Log metrics
        self.log('val_loss', val_loss, prog_bar=True)
        self.log('val_acc_step', self.accuracy(val_output, val_label), prog_bar=True)

        return {'val_loss': val_loss}

    def on_validation_epoch_end(self):
        # Modern replacement for validation_epoch_end
        self.log('val_acc_epoch', self.accuracy.compute(), prog_bar=True)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        return optimizer

    def forward(self, x):
        # in lightning, forward defines the prediction/inference actions
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.fc1(x)
        x = self.fc2(x)
        return F.softmax(x, dim=1)

"""### **Implement our Automatic Learning Rate Tuner**"""

# Create model instance
model = LitModel(batch_size=32, learning_rate=0.001)

# Setup callbacks
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    dirpath='./checkpoints',
    filename='dogs-cats-{epoch:02d}-{val_loss:.2f}',
    save_top_k=3,
    mode='min'
)

early_stopping_callback = EarlyStopping(
    monitor='val_loss',
    patience=5,
    mode='min'
)

# Learning Rate Logger with modern PyTorch Lightning syntax
trainer = pl.Trainer(
    max_epochs=10,
    callbacks=[checkpoint_callback, early_stopping_callback],
    accelerator='auto',  # Automatically select GPU if available
    devices='auto'       # Use all available devices
)

# Use the modern approach to finding the learning rate
# The auto_lr_find parameter has been moved to the Tuner's lr_find method
tuner = Tuner(trainer)
lr_finder = tuner.lr_find(model)

# Update the model's learning rate with the suggestion
if lr_finder.suggestion() is not None:
    new_lr = lr_finder.suggestion()
    print(f"Suggested learning rate: {new_lr}")
    model.learning_rate = new_lr
else:
    print("Learning rate finder didn't provide a suggestion. Using the default learning rate.")

# Train the model with the found learning rate
trainer.fit(model)

"""### **Visualize the LR vs Loss Plot**

The figure produced by lr_finder.plot() should look something like the figure below. It is recommended to not pick the learning rate that achieves the lowest loss, but instead something in the middle of the sharpest downward slope (red point). This is the point returned py lr_finder.suggestion().
"""

"""
Learning Rate Finder with plotting for PyTorch Lightning
"""
import pytorch_lightning as pl
from pytorch_lightning.tuner.tuning import Tuner
import matplotlib.pyplot as plt

# Create model instance
model = LitModel(batch_size=32, learning_rate=0.001)

# Setup callbacks
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    dirpath='./checkpoints',
    filename='dogs-cats-{epoch:02d}-{val_loss:.2f}',
    save_top_k=3,
    mode='min'
)

early_stopping_callback = EarlyStopping(
    monitor='val_loss',
    patience=5,
    mode='min'
)

# Create trainer without the learning rate finder flag
trainer = pl.Trainer(
    max_epochs=10,
    callbacks=[checkpoint_callback, early_stopping_callback],
    accelerator='auto',
    devices='auto'
)

# Create a tuner instance with our trainer
tuner = Tuner(trainer)

# Run the learning rate finder
lr_finder = tuner.lr_find(model)

# Plot the results
fig = lr_finder.plot(suggest=True)
plt.show()  # This will display the plot in Colab

# Get the suggested learning rate
suggested_lr = lr_finder.suggestion()
print(f"Suggested learning rate: {suggested_lr}")

# Update the model's learning rate
model.hparams.learning_rate = suggested_lr  # Update in hparams
model.learning_rate = suggested_lr  # Update the class attribute

# Train the model with the new learning rate
trainer.fit(model)

"""## **5. Train Model with learned Batch Size and Learning Rate**

The learning rate and batch size stored in `/content/lr_find_temp_model.ckpt`  and `/content/scale_batch_size_temp_model.ckpt` respectively, will be used over the learning rate set and batch sizes we set.

"""

# Import necessary libraries
import pytorch_lightning as pl
from pytorch_lightning.callbacks import TQDMProgressBar

# Initialize the model
model = LitModel(batch_size=32, learning_rate=0.001)

# Initialize a trainer with updated parameters for newer PyTorch Lightning versions
trainer = pl.Trainer(
    accelerator='auto',     # Replaces 'gpus=1', automatically selects GPU if available
    devices='auto',         # Use all available devices
    max_epochs=10,
    callbacks=[TQDMProgressBar(refresh_rate=10)]  # Replaces 'progress_bar_refresh_rate'
)

# Train the model ⚡
trainer.fit(model)

# After training, you can save the model
trainer.save_checkpoint("dogs_vs_cats_model.ckpt")

print("Training completed successfully!")
print(f"Final learning rate used: {model.learning_rate}")
print(f"Final batch size used: {model.batch_size}")

"""## **Tensorboard logs**"""

# Commented out IPython magic to ensure Python compatibility.
# Start tensorboard.
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

"""## **6. Using Callbacks - Early Stopping & Checkpointing**

**Early Stopping** -  early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent.

![](https://cdn-images-1.medium.com/max/920/1*iAK5uMoOlX1gZu-cSh1nZw.png)

**Model Checkpoint** - ModelCheckpoint callback is used to save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved.
"""

import os
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar
from pytorch_lightning.callbacks import Callback  # Import directly from callbacks

# Make sure the models directory exists
os.makedirs('models/', exist_ok=True)

# Setup Early Stopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    strict=False,
    verbose=False,
    mode='min'
)

# Setup Model Checkpoint
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    dirpath='models/',
    filename='sample-catsvsdogs-{epoch:02d}-{val_loss:.2f}',
    save_top_k=3,  # We save the top 3 models
    mode='min',
)

# We can even use some custom callbacks - Fixed to use the correct import
class MyPrintingCallback(Callback):  # Import Callback directly
    def on_init_start(self, trainer):
        print('Starting to init trainer!')

    def on_init_end(self, trainer):
        print('trainer is init now')

    def on_train_end(self, trainer, pl_module):
        print('do something when training ends')

"""### **Train with our Callbacks**"""

# init model
model = LitModel(batch_size=32, learning_rate=0.001)

# Initialize a trainer with updated parameters for newer PyTorch Lightning versions
trainer = pl.Trainer(
    accelerator='auto',    # Replaces 'gpus=1', automatically selects GPU if available
    devices='auto',        # Use all available devices
    max_epochs=10,
    callbacks=[
        TQDMProgressBar(refresh_rate=10),  # Replaces 'progress_bar_refresh_rate'
        early_stop,                        # Early stopping callback
        checkpoint_callback,              # Model checkpoint callback
        MyPrintingCallback()              # Our custom callback
    ]
)

# Train the model
trainer.fit(model)

# After training, we can check which checkpoints were saved
print("\nBest model checkpoints:")
for i, checkpoint in enumerate(checkpoint_callback.best_k_models.keys()):
    print(f"{i+1}. {checkpoint} (val_loss: {checkpoint_callback.best_k_models[checkpoint]:.4f})")

# Load the best model
best_model_path = checkpoint_callback.best_model_path
if best_model_path:
    print(f"\nLoading best model from: {best_model_path}")
    best_model = LitModel.load_from_checkpoint(best_model_path)
    print("Best model loaded successfully!")

# Commented out IPython magic to ensure Python compatibility.
# Start tensorboard.
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

"""## **8. Restore from Checkpoints**"""

# Get path of best model
checkpoint_callback.best_model_path

"""### **Load and run inference using the best checkpoint model**"""

#loading the best checkpoints to model
pretrained_model = LitModel.load_from_checkpoint(batch_size = 32, learning_rate=0.001, checkpoint_path = checkpoint_callback.best_model_path)
pretrained_model = pretrained_model.to("cuda")
pretrained_model.eval()
pretrained_model.freeze()

"""## **9. Save our Model for Production Deployments**

**Exporting to TorchScript**

TorchScript allows you to serialize your models in a way that it can be loaded in non-Python environments. The LightningModule has a handy method to_torchscript() that returns a scripted module which you can save or directly use.
"""

model = LitModel.load_from_checkpoint(batch_size = 32, learning_rate=0.001, checkpoint_path = checkpoint_callback.best_model_path)

script = model.to_torchscript()

# save for use in production environment
torch.jit.save(script, "model.pt")

"""## **10. Run inference on 32 images from our test data loder**"""

"""## **10. Run inference on 24 images from our test data loader**"""
import os
import torch
import torchvision
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader

# Make sure to use the best model for inference
# If you don't have the model loaded, load it from the checkpoint
best_model_path = checkpoint_callback.best_model_path if 'checkpoint_callback' in globals() else 'models/sample-catsvsdogs-best.ckpt'

# Check if the best model exists
if not os.path.exists(best_model_path):
    # Try to find any checkpoint in the models directory
    model_files = [f for f in os.listdir('models/') if f.endswith('.ckpt')] if os.path.exists('models/') else []
    # Also check lightning_logs directory for checkpoints
    lightning_logs = [f for f in os.listdir('lightning_logs/') if os.path.isdir(os.path.join('lightning_logs/', f))]
    for version in lightning_logs:
        checkpoints_dir = f'lightning_logs/{version}/checkpoints/'
        if os.path.exists(checkpoints_dir):
            model_files.extend([os.path.join(checkpoints_dir, f) for f in os.listdir(checkpoints_dir) if f.endswith('.ckpt')])

    if model_files:
        best_model_path = model_files[0]  # Just take the first one we find
        print(f"Found checkpoint: {best_model_path}")
    else:
        print("No checkpoints found. We'll use the model from the current session.")
        best_model_path = None

# Load the model for inference
if best_model_path and os.path.exists(best_model_path):
    print(f"Loading model from checkpoint: {best_model_path}")
    inference_model = LitModel.load_from_checkpoint(best_model_path)
else:
    print("Using the current model for inference")
    inference_model = model if 'model' in globals() else LitModel(batch_size=32, learning_rate=0.001)

# Make sure the model is in evaluation mode
inference_model.eval()

# Create the validation data loader if it doesn't exist
if 'val_loader' not in globals():
    # Check if val dataset exists
    if 'val' in globals():
        val_loader = DataLoader(dataset=val, batch_size=32, shuffle=False)
    else:
        # Recreate the validation dataset
        train_dir = './train'
        test_dir = './test1'

        # Get files in our directories
        train_files = os.listdir(train_dir)
        test_files = os.listdir(test_dir)

        # Create transforms
        from torchvision import transforms
        transformations = transforms.Compose([transforms.Resize((60, 60)), transforms.ToTensor()])

        # Create our test dataset
        from torch.utils.data import Dataset
        class CatDogDataset():
            def __init__(self, filelist, filepath, transform=None):
                self.filelist = filelist
                self.filepath = filepath
                self.transform = transform

            def __len__(self):
                return int(len(self.filelist))

            def __getitem__(self, index):
                imgpath = os.path.join(self.filepath, self.filelist[index])
                img = Image.open(imgpath)

                if "dog" in imgpath:
                    label = 1
                else:
                    label = 0

                if self.transform is not None:
                    img = self.transform(img)

                return (img, label)

        # Create validation dataset and loader
        val_dataset = CatDogDataset(test_files, test_dir, transformations)
        val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
inference_model = inference_model.to(device)

# Get a batch of data
print("Loading test images...")

data_iter = iter(val_loader)
images, labels = next(data_iter)
images = images.to(device)

# Run inference
print("Running inference...")
with torch.no_grad():
    outputs = inference_model(images[:24])
    predictions = torch.argmax(outputs, dim=1)

    # Convert to list
    pred_list = [p.item() for p in predictions]

# Display the results
print("Displaying results...")
fig = plt.figure(figsize=(12, 8))
fig.tight_layout()

# Define class names
class_names = {0: 'Cat', 1: 'Dog'}

# Plot images with their predictions
for i, (image, pred) in enumerate(zip(images[:24], pred_list)):
    plt.subplot(4, 6, i+1)
    plt.title(class_names[pred])
    plt.axis('off')

    # Convert tensor to numpy for plotting
    img_np = image.cpu().numpy()
    plt.imshow(np.transpose(img_np, (1, 2, 0)))

plt.subplots_adjust(wspace=0.3, hspace=0.3)
plt.show()

print("Inference completed!")

"""## **11. Multi-GPU Training**

To train on CPU/GPU/TPU without changing your code, we need to build a few good habits :)

Delete all `.cuda()` or `.to()` calls.

**Synchronize validation and test logging**

When running in distributed mode, we have to ensure that the validation and test step logging calls are synchronized across processes. This is done by adding sync_dist=True to all self.log calls in the validation and test step. This ensures that each GPU worker has the same behaviour when tracking model checkpoints, which is important for later downstream tasks such as testing the best checkpoint across all workers.

Note if you use any built in metrics or custom metrics that use the Metrics API, these do not need to be updated and are automatically handled for you.

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss(logits, y)
        # Add sync_dist=True to sync logging across all GPU workers
        self.log('validation_loss', loss, on_step=True, on_epoch=True, sync_dist=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss(logits, y)
        # Add sync_dist=True to sync logging across all GPU workers
        self.log('test_loss', loss, on_step=True, on_epoch=True, sync_dist=True)

There are other good practices that we don't use here, but they acn be found ehre - https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html
"""

import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchmetrics
from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar
from pytorch_lightning.callbacks import Callback
from pytorch_lightning.callbacks.early_stopping import EarlyStopping

class LitModel_mGPU(pl.LightningModule):
    def __init__(self, learning_rate=0.0005):
        super().__init__()
        self.save_hyperparameters()
        self.learning_rate = learning_rate

        # Fixed accuracy metrics with required 'task' parameter
        self.accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=2)
        self.train_acc = torchmetrics.Accuracy(task="multiclass", num_classes=2)
        self.valid_acc = torchmetrics.Accuracy(task="multiclass", num_classes=2)

        # Model architecture
        self.conv1 = nn.Sequential(nn.Conv2d(3,16,3), nn.ReLU(), nn.MaxPool2d(2,2))
        self.conv2 = nn.Sequential(nn.Conv2d(16,32,3), nn.ReLU(), nn.MaxPool2d(2,2))
        self.conv3 = nn.Sequential(nn.Conv2d(32,64,3), nn.ReLU(), nn.MaxPool2d(2,2))
        self.fc1 = nn.Sequential(nn.Flatten(), nn.Linear(64*5*5,256), nn.ReLU(), nn.Linear(256,128), nn.ReLU())
        self.fc2 = nn.Sequential(nn.Linear(128,2))

    def train_dataloader(self):
        # transforms
        return torch.utils.data.DataLoader(dataset=train, batch_size=32, shuffle=True)

    def val_dataloader(self):
        return torch.utils.data.DataLoader(dataset=val, batch_size=32, shuffle=False)

    def cross_entropy_loss(self, logits, labels):
        return F.nll_loss(logits, labels)

    def training_step(self, batch, batch_idx):
        data, label = batch
        output = self.forward(data)
        loss = nn.CrossEntropyLoss()(output, label)
        self.log('train_loss', loss, prog_bar=True)
        self.log('train_acc_step', self.accuracy(output, label), prog_bar=True)
        return {'loss': loss}

    def on_train_epoch_end(self):
        # Updated from training_epoch_end to on_train_epoch_end
        self.log('train_acc_epoch', self.accuracy.compute(), prog_bar=True)

    def validation_step(self, batch, batch_idx):
        val_data, val_label = batch
        val_output = self.forward(val_data)
        val_loss = nn.CrossEntropyLoss()(val_output, val_label)

        # Added sync_dist=True for multi-GPU training
        self.log('val_loss', val_loss, on_step=True, on_epoch=True, sync_dist=True, prog_bar=True)
        self.log('val_acc_step', self.accuracy(val_output, val_label), on_step=True, on_epoch=True, sync_dist=True, prog_bar=True)

        return {'val_loss': val_loss}

    def on_validation_epoch_end(self):
        # Updated from validation_epoch_end to on_validation_epoch_end
        self.log('val_acc_epoch', self.accuracy.compute(), sync_dist=True, prog_bar=True)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        return optimizer

    def forward(self, x):
        # in lightning, forward defines the prediction/inference actions
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.fc1(x)
        x = self.fc2(x)
        return F.softmax(x, dim=1)

"""###**Select GPU devices**
You can select the GPU devices using ranges, a list of indices or a string containing a comma separated list of GPU ids:
"""

# DEFAULT (int) specifies how many GPUs to use per node
#pl.Trainer(gpus=k)

# Above is equivalent to
#pl.Trainer(gpus=list(range(k)))

# Specify which GPUs to use (don't use when running on cluster)
#pl.Trainer(gpus=[0, 1])

# Equivalent using a string
#pl.Trainer(gpus='0, 1')

# To use all available GPUs put -1 or '-1'
# equivalent to list(range(torch.cuda.device_count()))
#pl.Trainer(gpus=-1)

"""#### **Note: In Colab we only have one GPU so this won't speed thigns up here**"""

class MyPrintingCallback(Callback):
    def on_init_start(self, trainer):
        print('Starting to init trainer!')

    def on_init_end(self, trainer):
        print('trainer is init now')

    def on_train_end(self, trainer, pl_module):
        print('do something when training ends')

"""#### **Setup checkpoint callback**"""
# Make sure the models directory exists
import os
os.makedirs('models/', exist_ok=True)

# Setup Model Checkpoint
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    dirpath='models/',
    filename='sample-catsvsdogs-{epoch:02d}-{val_loss:.2f}',
    save_top_k=3,  # We save the top 3 models
    mode='min',
)

"""#### **Training with multi-GPU support**"""
# init model
model = LitModel_mGPU(learning_rate=0.0005)

# Initialize a trainer with modern PyTorch Lightning syntax
trainer = pl.Trainer(
    accelerator="auto",    # Will use GPU if available, otherwise CPU
    devices="auto",        # Use all available devices
    max_epochs=10,
    callbacks=[
        TQDMProgressBar(refresh_rate=10),  # Modern progress bar
        EarlyStopping(monitor='val_loss', patience=3),
        checkpoint_callback,
        MyPrintingCallback()
    ],
    strategy="auto"  # Will automatically select the right strategy for multi-GPU
)

# Note for Colab users:
print("Note: In Colab we typically only have one GPU available, so multi-GPU training won't provide a speedup.")
print("However, this code is designed to work with multiple GPUs if they're available.")

# Train the model
trainer.fit(model)

# After training, print the best model
if checkpoint_callback.best_model_path:
    print(f"Best model saved at: {checkpoint_callback.best_model_path}")
    print(f"Best validation loss: {checkpoint_callback.best_model_score.item():.4f}")

"""## **12. Profiler - Performance and Bottleneck Profiler**"""

import pytorch_lightning as pl
from pytorch_lightning.callbacks import TQDMProgressBar
from pytorch_lightning.profilers import SimpleProfiler, AdvancedProfiler, PyTorchProfiler

# Initialize the model
model = LitModel_mGPU(learning_rate=0.0005)

# Create a profiler
simple_profiler = SimpleProfiler()

# For more detailed profiling, you could also use:
# advanced_profiler = AdvancedProfiler(dirpath=".", filename="advanced_profile_report")
# pytorch_profiler = PyTorchProfiler(dirpath=".", filename="pytorch_profile_report")

# Initialize a trainer with the profiler
trainer = pl.Trainer(
    accelerator="auto",        # Replaces 'gpus=1'
    devices="auto",            # Use available devices
    max_epochs=1,              # Just one epoch for profiling
    callbacks=[TQDMProgressBar(refresh_rate=10)],
    profiler=simple_profiler   # Use the simple profiler
)

# Train with profiling
print("Starting training with profiler enabled...")
trainer.fit(model)

# Print profiler report
print("\nProfiler Report:")
print("===============================")
print("This report shows the time spent in each function call.")
print("Use this to identify bottlenecks in your training pipeline.")
print("===============================")

# The profiler results will be automatically printed at the end of training
# You can also manually access the profiler report:
profiler_summary = simple_profiler.summary()
print(profiler_summary)

# For PyTorchProfiler, you can also get traces to visualize using Chrome Trace or TensorBoard
# if isinstance(simple_profiler, PyTorchProfiler):
#     simple_profiler.describe()

"""## **13. Training on TPUs**

**Tensor Processing Unit** is an AI accelerator application-specific integrated circuit developed by Google specifically for neural network machine learning.

**TPU Terminology**

A TPU is a Tensor processing unit. Each TPU has 8 cores where each core is optimized for 128x128 matrix multiplies. In general, a single TPU is about as fast as 5 V100 GPUs!

A TPU pod hosts many TPUs on it. Currently, TPU pod v2 has 2048 cores! You can request a full pod from Google cloud or a “slice” which gives you some subset of those 2048 cores.

### **Steps to Train on TPU**

1. Change runtime to TPU
2. Install PyTorch TPU
3. Re-install PyTorch Lightning and TorchMetrics (if needed) as therunetime has been reset.
"""

!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl
!pip install pytorch-lightning --quiet
!pip install torchmetrics

# Install requirements for TPU training
!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl
!pip install pytorch-lightning torchmetrics

# Import all packages we'll be using
import os
import torch
import torchmetrics
import torch.nn.functional as F

from torch import nn
from torchvision import transforms
from torch.utils.data import DataLoader, random_split

import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar
from PIL import Image

!gdown --id 1Dvw0UpvItjig0JbnzbTgYKB-ibMrXdxk
!unzip -q dogs-vs-cats.zip
!unzip -q train.zip
!unzip -q test1.zip

class Dataset():
    def __init__(self, filelist, filepath, transform = None):
        self.filelist = filelist
        self.filepath = filepath
        self.transform = transform

    def __len__(self):
        return int(len(self.filelist))

    def __getitem__(self, index):
        imgpath = os.path.join(self.filepath, self.filelist[index])
        img = Image.open(imgpath)

        if "dog" in imgpath:
            label = 1
        else:
            label = 0

        if self.transform is not None:
            img = self.transform(img)

        return (img, label)


# Set directory paths for our files
train_dir = './train'
test_dir = './test1'

# Get files in our directories
train_files = os.listdir(train_dir)
test_files = os.listdir(test_dir)

transformations = transforms.Compose([transforms.Resize((60,60)),transforms.ToTensor()])

# Create our train and test dataset objects
train = Dataset(train_files, train_dir, transformations)
val = Dataset(test_files, test_dir, transformations)

train, val = torch.utils.data.random_split(train,[20000,5000])

train_loader = torch.utils.data.DataLoader(dataset = train, batch_size = 32, shuffle=True)
val_loader = torch.utils.data.DataLoader(dataset = val, batch_size = 32, shuffle=False)

# Define our model with fixed Accuracy metrics
class LitModel_TPU(pl.LightningModule):
    def __init__(self, learning_rate=0.0005):
        super().__init__()
        self.save_hyperparameters()
        self.learning_rate = learning_rate

        # Fixed accuracy metrics with required 'task' parameter
        self.accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=2)
        self.train_acc = torchmetrics.Accuracy(task="multiclass", num_classes=2)
        self.valid_acc = torchmetrics.Accuracy(task="multiclass", num_classes=2)

        # Model architecture
        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3), nn.ReLU(), nn.MaxPool2d(2, 2))
        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 3), nn.ReLU(), nn.MaxPool2d(2, 2))
        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2, 2))
        self.fc1 = nn.Sequential(nn.Flatten(), nn.Linear(64*5*5, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU())
        self.fc2 = nn.Sequential(nn.Linear(128, 2))

    def train_dataloader(self):
        return train_loader

    def val_dataloader(self):
        return val_loader

    def cross_entropy_loss(self, logits, labels):
        return F.nll_loss(logits, labels)

    def training_step(self, batch, batch_idx):
        data, label = batch
        output = self.forward(data)
        loss = nn.CrossEntropyLoss()(output, label)

        # Log metrics
        self.log('train_loss', loss, prog_bar=True)
        self.log('train_acc_step', self.accuracy(output, label), prog_bar=True)

        return {'loss': loss}

    def on_train_epoch_end(self):
        # Updated from training_epoch_end to on_train_epoch_end
        self.log('train_acc_epoch', self.accuracy.compute(), prog_bar=True)

    def validation_step(self, batch, batch_idx):
        val_data, val_label = batch
        val_output = self.forward(val_data)
        val_loss = nn.CrossEntropyLoss()(val_output, val_label)

        # Added sync_dist=True for TPU training
        self.log('val_loss', val_loss, on_step=True, on_epoch=True, sync_dist=True, prog_bar=True)
        self.log('val_acc_step', self.accuracy(val_output, val_label), on_step=True, on_epoch=True, sync_dist=True, prog_bar=True)

        return {'val_loss': val_loss}

    def on_validation_epoch_end(self):
        # Updated from validation_epoch_end to on_validation_epoch_end
        self.log('val_acc_epoch', self.accuracy.compute(), sync_dist=True, prog_bar=True)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        return optimizer

    def forward(self, x):
        # in lightning, forward defines the prediction/inference actions
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.fc1(x)
        x = self.fc2(x)
        return F.softmax(x, dim=1)

# Set up callbacks
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    strict=False,
    verbose=True,
    mode='min'
)

checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    dirpath='./models/',
    filename='tpu-dogs-cats-{epoch:02d}-{val_loss:.2f}',
    save_top_k=3,
    mode='min',
)

# Initialize our model
model = LitModel_TPU(learning_rate=0.0005)

# Initialize a trainer for TPUs with updated parameters for newer PyTorch Lightning versions
# and fixed precision type for TPU
trainer = pl.Trainer(
    accelerator="tpu",     # Use TPU
    devices=8,             # Use 8 TPU cores
    max_epochs=1,
    callbacks=[
        TQDMProgressBar(refresh_rate=10),
        early_stop,
        checkpoint_callback
    ],
    precision="16-true",   # Fixed: TPU supports '32-true', '16-true', or 'bf16-true'
)

print("Starting TPU training...")
print("Note: This will only work if you're running in a TPU runtime environment.")
print("If you're not in a TPU environment, you'll get an error.")

# Train the model
try:
    trainer.fit(model)
    print("Training completed successfully!")

    # Show best model information
    if checkpoint_callback.best_model_path:
        print(f"Best model saved at: {checkpoint_callback.best_model_path}")
        print(f"Best validation loss: {checkpoint_callback.best_model_score.item():.4f}")
except Exception as e:
    print(f"Error during training: {str(e)}")
    print("\nIf you're not running in a TPU environment, try using:")
    print("trainer = pl.Trainer(")
    print("    accelerator='auto',  # Will use GPU if available, otherwise CPU")
    print("    devices='auto',      # Use all available devices")
    print("    precision='16-mixed', # Mixed precision for GPUs")
    print("    # Other parameters remain the same")
    print(")")

