# -*- coding: utf-8 -*-
"""20. PyTorch Lightning - Transfer Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_DPYjbej-UM6H8HsRSXbeZ4owKVGPBmd

# **PyTorch Lightning - Transfer Learning**
---
https://pytorch-lightning.readthedocs.io/en/1.2.0/advanced/transfer_learning.html
---

In this lesson, we learn to use the amazing library **PyTorch Lightning**. It's a great way to organize. your PyTorch code and get many great features and added benefits. We'll be doing the following in this guide:
1. Setup and Install Lightning
2. Create our Lightning Model Class and use a pre-trained model
3. Train our model

## **1. Setup and Install Lightning**
"""

# First we install PyTorch Lightning and TorchMetrics√ü
!pip install pytorch-lightning --quiet
!pip install torchmetrics

import os
import torch
import torchmetrics
import torch.nn.functional as F
import torchvision.models as models

from torch import nn
from torchvision import transforms
from torch.utils.data import DataLoader, random_split
from PIL import Image

import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar

"""#### **Download our datasets**"""

!gdown --id 1Dvw0UpvItjig0JbnzbTgYKB-ibMrXdxk
!unzip -q dogs-vs-cats.zip
!unzip -q train.zip
!unzip -q test1.zip

# Enable GPU if available (Colab-specific)
!nvidia-smi
# Connect to high-RAM backend
import google.colab
google.colab.output.enable_custom_widget_manager()

# Check if we can use GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Free up memory if using GPU
if torch.cuda.is_available():
    torch.cuda.empty_cache()

"""## **Setup our Dataloaders**"""

# Super simple Dataset class
class DogCatDataset:
    def __init__(self, filelist, filepath, transform=None):
        self.filelist = filelist
        self.filepath = filepath
        self.transform = transform
        # Precompute labels
        self.labels = [1 if "dog" in f else 0 for f in filelist]

    def __len__(self):
        return len(self.filelist)

    def __getitem__(self, index):
        # Load and transform image
        img_path = os.path.join(self.filepath, self.filelist[index])
        img = Image.open(img_path).convert('RGB')  # Force RGB

        if self.transform:
            img = self.transform(img)

        return img, self.labels[index]

# Fast data transform pipeline
transforms_train = transforms.Compose([
    transforms.Resize((160, 160)),  # Smaller than original for speed
    transforms.RandomHorizontalFlip(),  # Data augmentation
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

"""## **2.Creating our Lightning Module using a Pre-trained (ImageNet) Model**

Using pre-trained models for Transfer Learning is simple!

All we do is load the model weights in the init function. Here we use **resNet50**.
"""

# Simplified Lightning Module with MobileNetV2
class FastTransferLearning(pl.LightningModule):
    def __init__(self, lr=5e-4):
        super().__init__()
        self.lr = lr

        # Use lightweight MobileNetV2 instead of ResNet
        backbone = models.mobilenet_v2(weights='IMAGENET1K_V1')

        # Freeze most of the network
        for param in backbone.features[:-3].parameters():
            param.requires_grad = False

        # Replace classifier
        backbone.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(backbone.last_channel, 2)
        )

        self.model = backbone
        self.accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=2)

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)

        # Log only occasionally to save time
        if batch_idx % 20 == 0:
            self.log('train_loss', loss, prog_bar=True)
            self.log('train_acc', self.accuracy(logits, y), prog_bar=True)

        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)

        self.log('val_loss', loss, prog_bar=True)
        self.log('val_acc', self.accuracy(logits, y), prog_bar=True)

        return loss

    def configure_optimizers(self):
        # Use faster optimizer with momentum
        optimizer = torch.optim.SGD(
            self.parameters(),
            lr=self.lr,
            momentum=0.9,
            weight_decay=1e-4
        )

        # One Cycle LR scheduler - fast convergence
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=self.lr,
            total_steps=self.trainer.estimated_stepping_batches,
            pct_start=0.1
        )

        return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "interval": "step"}}

"""## **Start Training - Exactly the same as before**"""

# Main execution function
def run_fast_training():
    # Set base directories
    train_dir = './train'
    test_dir = './test1'

    # Get only image files
    train_files = [f for f in os.listdir(train_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]

    # Quick dataset creation
    full_dataset = DogCatDataset(train_files, train_dir, transforms_train)

    # Split dataset - use more data for training
    train_size = int(0.85 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_set, val_set = random_split(full_dataset, [train_size, val_size])

    # Optimized data loaders for Colab
    train_loader = DataLoader(
        train_set,
        batch_size=128,  # Large batch size
        shuffle=True,
        num_workers=2,   # Colab works well with 2
        pin_memory=True
    )

    val_loader = DataLoader(
        val_set,
        batch_size=256,  # Even larger for validation
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    # Simple callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=2),
        ModelCheckpoint(
            dirpath='./models/',
            filename='mobilenet-{epoch:02d}-{val_loss:.2f}',
            monitor='val_loss',
            save_top_k=1
        )
    ]

    # Create model
    model = FastTransferLearning()

    # Create trainer with speed optimizations
    trainer = pl.Trainer(
        max_epochs=3,               # Fewer epochs
        accelerator='auto',         # Auto-detect GPU
        precision='16-mixed',       # Mixed precision
        gradient_clip_val=0.5,      # Prevent gradient explosion
        accumulate_grad_batches=2,  # Effective batch size doubled
        log_every_n_steps=50,       # Less logging
        callbacks=callbacks,
        enable_progress_bar=True,
        enable_model_summary=True,
        deterministic=False,        # Faster but less reproducible
    )

    # Start training
    print("Starting fast training...")
    trainer.fit(model, train_loader, val_loader)

    # Report results
    print("\nTraining completed!")
    if callbacks[1].best_model_path:
        print(f"Best model: {callbacks[1].best_model_path}")
        print(f"Val loss: {callbacks[1].best_model_score:.4f}")

    return model

# Run everything
if __name__ == "__main__":
    # Run training
    model = run_fast_training()

    # Save the final model in a Colab-friendly way
    torch.save(model.state_dict(), 'final_model.pt')
    print("Model saved to final_model.pt")

    # Optional: Download the model to your local machine
    from google.colab import files
    files.download('final_model.pt')

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

